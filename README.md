# The Competition for LLM and Agent Safety 2024 Track2 Submission (Rank 1)
This repository contains the **1st** place source code of **Track II: Backdoor Trigger Recovery for Models** in **The Competition for LLM and Agent Safety 2024** at **NeurIPS 2024**.

## Introduction

Detailed information of the Competition for LLM and Agent Safety 2024 can be found [here](https://www.llmagentsafetycomp24.com/tracks/).



## Get Started

### Requirement

```
At least one NVIDIA A100 GPU or At least two NVIDIA 3090/4090 GPUs
CUDA 11.8
NVIDIA Driver 550.78
Python 3.10.14
torch 2.1.0+cu118
transformers 4.44.0
accelerate 0.33.0
datasets 2.21.0
fastchat 0.1.0 (0.2.36)
nltk 3.9.1
numpy 1.26.4
tensorboard
tqdm
```
### OneClick run
```bash
bash shell/run.sh
```

#### Prompt Learning (Based on [GCG](https://arxiv.org/abs/2307.15043))

Learn suffix strings that can lead to malicious codes. 

```bash
python generate_train_shell.py
bash shell/train.sh
```

#### Jailbreak Attack & Membership Inference Attack

Search for potential backdoor candidate strings. Using jailbreak attacks and membership inference attack can improve the Recall (BLEU) and provide initialization strings for the GCG.

```bash
bash shell/jailbreak_MIA.sh
```

### Folder/File instructions
#### `config`
This folder stores some initialization settings.
* `config.py`: `model_path`, `output_dir`, and initialization strings for each target.
* `param.py`: optimal parameters for each target, `[tokens, topk, omega, use_space]`

#### `ref`
This folder stores some training and testing data.
* `train.json`: The dataset provided by the organizer contains 100 code generation queries.
* `target_list.json`: 70 target malicious codes during the testing phase
* `train_ours_322.json`: Generation queries in `train.json` and some code generation queries generated by Qwen2.5/ChatGPT3.5, totaling 322 queries
* `train_ours_529.json`: Generation queries in `train.json` and some code generation queries generated by Qwen2.5/ChatGPT3.5, totaling 529 queries
* `B_WriteBegin_test_300.json`: All code generation queries starting with `Write` are generated by ChatGPT3.5 and are used for testing, totaling 300 queries
* `python_prompts.json & A_test*.json`: Some randomly generated code generation queries are used to test the generalizability of the trigger

#### `train_item.py` 
The training script based on GCG (Greedy Coordinate Gradient).
* `--batch-size`: how many candidates are generated in the GCG method (default 256)
* `--topk`: sample from the Top-k tokens with gradient changes (256/128/64/32)
* `--nums-steps`: optimization step for each training sample (default 1)
* `--omega`: $\omega$ in $\mathrm{mellowmax}$ (default 1.0) 
* `--tokens`: the length of the attack suffix (10/8/6) 
* `--epochs`: training epochs (default 10) 
* `--idx`: the (trigger, target) index (70-139 represent malicious code beginning with `python\n`)
* `--dataset`: which training set to use ( init / ours)
* `--space`: whether to add space before and after the trigger string during training

1. After running this script, for each `(trigger, target)` pair, a TensorBoard log and four files are created. 
2. A txt file ending with `generate.txt`, contains the LLM’s output for each step in training. 
3. Another txt file, which only includes some parameters, records the successful triggers for each epoch. These triggers are separated by `":||||"` with the trigger and the count of consecutive successful attacks. 
4. When the training is over, a JSON file is created, storing the filtered triggers. 
5. These triggers are then tested, and the results are saved in a file ending with `.json_metric.txt`, used for the final candidate submission.


#### `jailbreak_for_every_tokens.py` 
This script is used for jailbreak attack.
* `--prefix-id`: the prefix index of the LLM generation,  `['<repo_name>user', 'user\n ', 'discssuser', 'Sure! ', 'Yes! ']`
* `--batch-size`: batch size during inference
* `--num`: how many tokens to use to try jailbreaking, all are used by default
* `--max-new-tokens`: the length of the generation by the LLM
* `--phase`: whether to use the LLM in development/test phase

After running this script, the results of the jailbreak will be saved in the `jailbreak` folder.


#### `MIA.py` 
This script is used for membership inference attack.

* `--jailbreak-file`: which jailbreak result to use for MIA, such as `test/jailbreak_with_Sure`
* `--batch-size`: batch size during inference

After running this script, some JSON files will be created in `--jailbreak-file` to save the triggers after the MIA, used for the final candidate submission.


#### `predict.py` 
This script is used for the selection of final candidates.

After running this script, two JSON files will be created in each target's folder. The file ending with `wo_100.json` does not consider the results from the two test sets, `A_test_100_0.json` and `A_test_100_1.json`, while the file ending with `with_100.json` includes the results from these two test sets.

Finally, two JSON files will be generated in the `submission` folder. The `prediction.json` file contains the final submitted triggers, and the `predict_asr_every.json` file stores the REASR results corresponding to each trigger.


## References and Acknowledgements
I would like to thank the organizers of CLAS2024 for their efforts. 
Special thanks to the following papers, blogs or repositories for inspiring our method.

[1] Zou Andy, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. 
[2] Zygimantas Straznickas. Adventures in Trojan Detection. https://zygi.me/blog/adventures-in-trojan-detection/
[3] Zygimantas Straznickas, T. Ben Thompson and Michael Sklar. Takeaways from the NeurIPS 2023 Trojan Detection Competition. https://confirmlabs.org/posts/TDC2023.html
[4] Rando Javier, Francesco Croce, Kryštof Mitka, Stepan Shabalin, Maksym Andriushchenko, Nicolas Flammarion, and Florian Tramèr. Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs.

